{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd5d0b-2159-4018-97da-939587bfdd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2\n",
    "from lifelines import CoxPHFitter\n",
    "from typing import List, Tuple, Union\n",
    "from pycox.evaluation import EvalSurv\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "Numeric = Union[float, int]\n",
    "NumericArrayLike = Union[List[Numeric], Tuple[Numeric], np.array]\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ResPath = './run-results'\n",
    "# all_methods = os.listdir(ResPath)\n",
    "all_methods = ['FGCNSurv', 'M2EFM', 'Multimodal_NSCLC', 'SALMON', 'GDP', 'MiNet', 'SurvivalNet', 'SAE', 'CSAE',\n",
    "               'CustOmics',\n",
    "               'I-Boost', 'TCGA-omics-integration', 'MDNNMD', 'TF-LogHazardNet', 'TF-ESN', 'OmiEmbed', 'IPF-LASSO',\n",
    "               'Priority-Lasso',\n",
    "               'blockForest', 'MultimodalSurvivalPrediction']\n",
    "\n",
    "## methods output Hazard Ratio\n",
    "HR_methods = ['IPF-LASSO', 'M2EFM', 'Multimodal_NSCLC', 'SALMON', 'GDP', 'MiNet', 'SurvivalNet', 'SAE', 'CSAE',\n",
    "              'CustOmics',\n",
    "              'MultimodalSurvivalPrediction', 'FGCNSurv', 'I-Boost', 'Priority-Lasso', 'TCGA-omics-integration']\n",
    "## methods output Survival Probability\n",
    "SP_methods = ['TF-LogHazardNet', 'TF-ESN', 'OmiEmbed', 'blockForest']\n",
    "## methods output Vital Status with Probability\n",
    "VP_methods = ['MDNNMD']\n",
    "\n",
    "\n",
    "#### DEFINE FUNCTIONS\n",
    "## function to find indices for specific times\n",
    "def find_idx(times, all_times):\n",
    "    idx_vec = []\n",
    "    for time in times:\n",
    "        temp_times = np.abs(all_times - time)\n",
    "        idx = np.where(temp_times == np.min(temp_times))[0]\n",
    "        idx_vec.append(idx[0])\n",
    "\n",
    "    return idx_vec\n",
    "\n",
    "\n",
    "## calculate the survival probability using hazard ratio\n",
    "def cal_survprob(pred_train, pred_val):\n",
    "    pred_train['log_HR'] = pred_train['predTrain'].apply(np.log)\n",
    "\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(pred_train.loc[:, ['log_HR', 'time', 'status']], duration_col='time', event_col='status')\n",
    "    baseline_cum_hazard = cph.baseline_cumulative_hazard_\n",
    "\n",
    "    all_times = baseline_cum_hazard.index.values\n",
    "    max_time = np.max(all_times)\n",
    "    target_times = np.linspace(0, max_time, 20).tolist()\n",
    "    baseline_haz_indices = find_idx(target_times, all_times)\n",
    "    baseline_haz = baseline_cum_hazard.iloc[baseline_haz_indices, 0].values\n",
    "\n",
    "    HR_val = pred_val['predVal'].to_numpy()\n",
    "    CHF_val = HR_val[:, np.newaxis] * baseline_haz\n",
    "    survprob_val = np.exp(-CHF_val)\n",
    "    survprob_val = survprob_val.T\n",
    "    survprob_val = pd.DataFrame(survprob_val)\n",
    "    survprob_val.index = target_times\n",
    "\n",
    "    return survprob_val\n",
    "\n",
    "\n",
    "## calculate time-dependent CIndex and IBS\n",
    "def cal_tdci_ibs(surv_test, duration_test, event_test):\n",
    "    ev = EvalSurv(surv_test, duration_test, event_test, censor_surv='km')\n",
    "    testci = ev.concordance_td('antolini')\n",
    "\n",
    "    max_time = np.max(duration_test)\n",
    "    time_grid = np.linspace(0, max_time, 20)\n",
    "\n",
    "    ibs = ev.integrated_brier_score(time_grid)\n",
    "    return testci, ibs\n",
    "\n",
    "\n",
    "## calculate risk score using survival probability\n",
    "def cal_ci(predRes):\n",
    "    risk_score = -np.log(predRes)\n",
    "    risk_score = np.sum(risk_score, axis=1)\n",
    "    risk_score = risk_score / np.max(risk_score)\n",
    "    return risk_score\n",
    "\n",
    "\n",
    "## calculate D-Calibration (Paper: Effective Ways to Build and Evaluate Individual Survival Distributions)\n",
    "## (I think it can be considered as Moderate Calibration)\n",
    "# Some necessary functions\n",
    "def check_indicators(indicators: np.array) -> None:\n",
    "    if not all(np.logical_or(indicators == 0, indicators == 1)):\n",
    "        raise ValueError(\n",
    "            \"Event indicators must be 0 or 1 where 0 indicates censorship and 1 is an event.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def to_array(array_like: NumericArrayLike, to_boolean: bool = False) -> np.array:\n",
    "    array = np.asarray(array_like)\n",
    "    shape = np.shape(array)\n",
    "    if len(shape) > 1:\n",
    "        raise ValueError(\n",
    "            f\"Input should be a 1-d array. Got a shape of {shape} instead.\"\n",
    "        )\n",
    "    if np.any(array < 0):\n",
    "        raise ValueError(\"All event times must be greater than or equal to zero.\")\n",
    "    if to_boolean:\n",
    "        check_indicators(array)\n",
    "        return array.astype(bool)\n",
    "    return array\n",
    "\n",
    "\n",
    "# get the survival probability for each patient at the time they died or were censored\n",
    "def get_1surv_prob(pred_df, survival_times):\n",
    "    time_points = pred_df.index.values\n",
    "    probabilities = np.zeros(len(survival_times))\n",
    "\n",
    "    for i, time in enumerate(survival_times):\n",
    "        distances = np.abs(time_points - time)\n",
    "        min_distance = np.min(distances)\n",
    "        candidate_indices = np.where(distances == min_distance)[0]\n",
    "\n",
    "        if len(candidate_indices) == 1:\n",
    "            selected_idx = candidate_indices[0]\n",
    "        else:\n",
    "            candidate_probs = pred_df.iloc[candidate_indices, i]\n",
    "            min_prob = candidate_probs.min()\n",
    "            min_prob_indices = candidate_indices[candidate_probs == min_prob]\n",
    "            selected_idx = min_prob_indices[0]\n",
    "        probabilities[i] = pred_df.iloc[selected_idx, i]\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "\n",
    "# function to calculate D-Calibration\n",
    "def d_calibration(\n",
    "        event_indicators: NumericArrayLike,\n",
    "        predictions: NumericArrayLike,\n",
    "        bins: int = 10,\n",
    ") -> dict:\n",
    "    event_indicators = to_array(event_indicators, to_boolean=True)\n",
    "    predictions = to_array(predictions)\n",
    "\n",
    "    # include minimum to catch if probability = 1.\n",
    "    bin_index = np.minimum(np.floor(predictions * bins), bins - 1).astype(int)\n",
    "    censored_bin_indexes = bin_index[~event_indicators]\n",
    "    uncensored_bin_indexes = bin_index[event_indicators]\n",
    "\n",
    "    censored_predictions = predictions[~event_indicators]\n",
    "    censored_contribution = 1 - (censored_bin_indexes / bins) * (\n",
    "            1 / censored_predictions\n",
    "    )\n",
    "    censored_following_contribution = 1 / (bins * censored_predictions)\n",
    "\n",
    "    contribution_pattern = np.tril(np.ones([bins, bins]), k=-1).astype(bool)\n",
    "\n",
    "    following_contributions = np.matmul(\n",
    "        censored_following_contribution, contribution_pattern[censored_bin_indexes]\n",
    "    )\n",
    "    single_contributions = np.matmul(\n",
    "        censored_contribution, np.eye(bins)[censored_bin_indexes]\n",
    "    )\n",
    "    uncensored_contributions = np.sum(np.eye(bins)[uncensored_bin_indexes], axis=0)\n",
    "    bin_count = (\n",
    "            single_contributions + following_contributions + uncensored_contributions\n",
    "    )\n",
    "    chi2_statistic = np.sum(\n",
    "        np.square(bin_count - len(predictions) / bins) / (len(predictions) / bins)\n",
    "    )\n",
    "    return dict(\n",
    "        p_value=1 - chi2.cdf(chi2_statistic, bins - 1),\n",
    "        bin_proportions=bin_count / len(predictions),\n",
    "        censored_contributions=(single_contributions + following_contributions)\n",
    "                               / len(predictions),\n",
    "        uncensored_contributions=uncensored_contributions / len(predictions),\n",
    "    )\n",
    "\n",
    "\n",
    "## function to create table for each metric\n",
    "def create_metric_table(data, metric, datasets):\n",
    "    metric_data = {method: values[metric] for method, values in data.items()}\n",
    "    df = pd.DataFrame(metric_data, index=[f'Run {i + 1}' for i in range(len(datasets))])\n",
    "    df.index = datasets\n",
    "    return df\n",
    "\n",
    "\n",
    "## create summary table for all three metrics\n",
    "def create_summary_table(cindex_table, tdcindex_table, ibs_table, cal_table):\n",
    "    summary_data = {\n",
    "        'Cindex': cindex_table.mean(),\n",
    "        'tdCindex': tdcindex_table.mean(),\n",
    "        'IBS': ibs_table.mean(),\n",
    "        'Calibration': cal_table.mean()\n",
    "    }\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "#### CALCULATE METRICS FOR ALL METHODS\n",
    "allRes = {}\n",
    "for method in all_methods:\n",
    "    all_datasets = os.listdir(os.path.join(ResPath, method))\n",
    "    all_datasets = [x for x in all_datasets if \"TCGA-\" in x]\n",
    "    all_datasets = sorted(all_datasets)  # Returns new sorted list\n",
    "    allCIndex = []\n",
    "    alltdCIndex = []\n",
    "    allCal = []\n",
    "    allIBS = []\n",
    "\n",
    "    for dataset in all_datasets:\n",
    "        NewPath = os.path.join(ResPath, method, dataset)\n",
    "        CIndex = []\n",
    "        tdCIndex = []\n",
    "        IBS = []\n",
    "        Cal = []\n",
    "\n",
    "        for time in range(1, 6):\n",
    "            for fold in range(1, 11):\n",
    "                TrainRes = pd.read_csv(os.path.join(ResPath, method, dataset, \"Time\" + str(time)) + \"/Train_Res_\" + str(fold) + \".csv\",\n",
    "                                       header=0, index_col=0)\n",
    "                ValRes = pd.read_csv(os.path.join(ResPath, method, dataset, \"Time\" + str(time)) + \"/Val_Res_\" + str(fold) + \".csv\", header=0,\n",
    "                                     index_col=0)\n",
    "                trueDat = np.array(ValRes[['time', 'status']])\n",
    "\n",
    "                if method in HR_methods:\n",
    "                    try:\n",
    "                        valSurvProb = cal_survprob(TrainRes, ValRes)\n",
    "                        tmp_tdCI, tmp_ibs = cal_tdci_ibs(valSurvProb, trueDat[:, 0], trueDat[:, 1])\n",
    "                        tmp_survprob = get_1surv_prob(valSurvProb, trueDat[:, 0])\n",
    "                        tmp_Cal = d_calibration(trueDat[:, 1], tmp_survprob)[\"p_value\"]\n",
    "                        # tmp_Cal = -np.log10(tmp_Cal)     ### transform p-values into -log10\n",
    "\n",
    "                    except:\n",
    "                        tmp_tdCI = np.NaN\n",
    "                        tmp_ibs = np.NaN\n",
    "                        tmp_Cal = np.NaN\n",
    "\n",
    "                    try:\n",
    "                        tmp_CI = concordance_index_censored(event_indicator=ValRes['status'].values.astype(bool),\n",
    "                                                            event_time=ValRes['time'].values,\n",
    "                                                            estimate=ValRes['predVal'])[0]\n",
    "                    except:\n",
    "                        tmp_CI = np.NaN\n",
    "\n",
    "                if method in SP_methods:\n",
    "                    predVal = ValRes.drop(['time', 'status'], axis=1)\n",
    "                    try:\n",
    "                        riskScore = cal_ci(predVal.to_numpy())\n",
    "                        tmp_CI = concordance_index_censored(event_indicator=ValRes['status'].values.astype(bool),\n",
    "                                                            event_time=ValRes['time'].values,\n",
    "                                                            estimate=riskScore)[0]\n",
    "                    except:\n",
    "                        tmp_CI = np.NaN\n",
    "                    time_points = [int(col.split('_')[-1]) for col in predVal.columns if '_' in col]\n",
    "                    valSurvProb = predVal.T\n",
    "                    valSurvProb = valSurvProb.reset_index(drop=True)\n",
    "                    valSurvProb.index = time_points\n",
    "\n",
    "                    try:\n",
    "                        tmp_tdCI, tmp_ibs = cal_tdci_ibs(valSurvProb, trueDat[:, 0], trueDat[:, 1])\n",
    "                        tmp_survprob = get_1surv_prob(valSurvProb, trueDat[:, 0])\n",
    "                        tmp_Cal = d_calibration(trueDat[:, 1], tmp_survprob)[\"p_value\"]\n",
    "                        # tmp_Cal = -np.log10(tmp_Cal)  ### transform p-values into -log10\n",
    "\n",
    "                    except:\n",
    "                        tmp_tdCI = np.NaN\n",
    "                        tmp_ibs = np.NaN\n",
    "                        tmp_Cal = np.NaN\n",
    "\n",
    "                if method in VP_methods:\n",
    "                    try:\n",
    "                        tmp_CI = concordance_index_censored(event_indicator=ValRes['status'].values.astype(bool),\n",
    "                                                            event_time=ValRes['time'].values,\n",
    "                                                            estimate=1 - ValRes['predVal'].to_numpy())[0]\n",
    "                    except:\n",
    "                        tmp_CI = np.NaN\n",
    "\n",
    "                    tmp_tdCI = np.NaN\n",
    "                    tmp_ibs = np.NaN\n",
    "                    tmp_Cal = np.NaN\n",
    "\n",
    "                if tmp_Cal > 0.05:\n",
    "                    tmp_Cal = 1\n",
    "                else:\n",
    "                    tmp_Cal = 0\n",
    "\n",
    "                CIndex.append(tmp_CI)\n",
    "                tdCIndex.append(tmp_tdCI)\n",
    "                IBS.append(tmp_ibs)\n",
    "                Cal.append(tmp_Cal)\n",
    "\n",
    "        CIndex = np.nanmean(CIndex)\n",
    "        tdCIndex = np.nanmean(tdCIndex)\n",
    "        IBS = np.nanmean(IBS)\n",
    "        # Cal = np.nanmean(Cal)\n",
    "        Cal = np.nansum(Cal)\n",
    "\n",
    "\n",
    "        allCIndex.append(CIndex)\n",
    "        alltdCIndex.append(tdCIndex)\n",
    "        allIBS.append(IBS)\n",
    "        allCal.append(Cal)\n",
    "\n",
    "    # allCIndex = np.nanmean(allCIndex)\n",
    "    # alltdCIndex = np.nanmean(alltdCIndex)\n",
    "    # allIBS = np.nanmean(allIBS)\n",
    "    allRes[method] = {'CIndex': allCIndex, 'tdCIndex': alltdCIndex, 'IBS': allIBS, 'Cal': allCal}\n",
    "\n",
    "\n",
    "CIndex_Table = create_metric_table(allRes, 'CIndex', all_datasets)\n",
    "tdCIndex_Table = create_metric_table(allRes, 'tdCIndex', all_datasets)\n",
    "IBS_Table = create_metric_table(allRes, 'IBS', all_datasets)\n",
    "Cal_Table = create_metric_table(allRes, 'Cal', all_datasets)\n",
    "Res_Table = create_summary_table(CIndex_Table, tdCIndex_Table, IBS_Table, Cal_Table)\n",
    "\n",
    "CIndex_Table.to_csv(ResPath + \"/CIndex_Table.csv\", sep=\",\", header=True)\n",
    "tdCIndex_Table.to_csv(ResPath + \"/tdCIndex_Table.csv\", sep=\",\", header=True)\n",
    "IBS_Table.to_csv(ResPath + \"/IBS_Table.csv\", sep=\",\", header=True)\n",
    "Cal_Table.to_csv(ResPath + \"/Calibration_Table.csv\", sep=\",\", header=True)\n",
    "Res_Table.to_csv(ResPath + \"/Res_Summary_Table.csv\", sep=\",\", header=True)\n",
    "\n",
    "print(Res_Table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rp-review-env2",
   "language": "python",
   "name": "rp-review-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
